[core]
# The folder where your pipelines live
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
remote_logging = False

# The executor class that airflow should use
executor = CeleryExecutor

# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the examples that ship with Airflow
load_examples = False

# Whether to load the default connections that ship with Airflow
load_default_connections = True

[database]
# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow

# The SqlAlchemy pool size
sql_alchemy_pool_size = 5

# The SqlAlchemy pool recycle
sql_alchemy_pool_recycle = 1800

[celery]
# The Celery broker URL
broker_url = redis://:@redis:6379/0

# The Celery result backend
result_backend = db+postgresql://airflow:airflow@postgres/airflow

# Celery Flower is a sweet UI for Celery
flower_basic_auth = admin:admin

# Default queue that tasks get assigned to and that worker listen on
default_queue = default

[celery_broker_transport_options]
# The visibility timeout defines the number of seconds to wait for the worker
visibility_timeout = 21600

[scheduler]
# Task instances listen for external kill signal (when you clear tasks from the CLI or the UI)
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks
scheduler_heartbeat_sec = 5

# The number of seconds to wait between consecutive DAG file processing
min_file_process_interval = 30

# Number of seconds after which a DAG file is parsed
dag_dir_list_interval = 300

# How often should stats be printed to the logs
print_stats_interval = 30

# How often (in seconds) to scan the DAGs directory for new files
catchup_by_default = True

# Maximum number of threads that can be used to parse DAG files
max_threads = 2

[webserver]
# The base url of your website
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Number of seconds the webserver waits before killing gunicorn master that doesn't respond
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to refresh at a time
worker_refresh_batch_size = 1

# Number of workers to run the Gunicorn web server
workers = 4

# The worker class gunicorn should use
worker_class = sync

# Expose the configuration file in the web server
expose_config = True

# Set to true to turn on authentication
authenticate = False

[email]
email_backend = airflow.utils.email.send_email_smtp

[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_user = airflow
smtp_password = airflow
smtp_port = 25
smtp_mail_from = airflow@example.com

[kubernetes]
# The name of the Kubernetes namespace Airflow should operate in
namespace = airflow

[logging]
# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely
remote_logging = False

# Logging level
logging_level = INFO

# Logging class
logging_config_class =

# Format of Log line
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

[metrics]
# StatsD (https://github.com/etsy/statsd) integration settings
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

[api]
# How to authenticate users of the API
auth_backend = airflow.api.auth.backend.default

[admin]
# UI to hide sensitive variable fields when set to True
hide_sensitive_variable_fields = True
