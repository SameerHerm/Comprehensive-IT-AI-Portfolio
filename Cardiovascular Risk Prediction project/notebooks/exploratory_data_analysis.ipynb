{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardiovascular Risk Prediction - Exploratory Data Analysis\n",
    "\n",
    "This notebook explores the cardiovascular dataset and provides insights for model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_path = '../data/cardiovascular_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "df.info()\n",
    "\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(\"=\"*50)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Percentage', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(\"Missing Values:\")\n",
    "    print(missing_df)\n",
    "    \n",
    "    # Visualize missing values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_df['Percentage'].plot(kind='barh')\n",
    "    plt.xlabel('Percentage (%)')\n",
    "    plt.title('Missing Values by Feature')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable (TenYearCHD)\n",
    "target_col = 'TenYearCHD'\n",
    "\n",
    "if target_col in df.columns:\n",
    "    target_counts = df[target_col].value_counts()\n",
    "    target_percentages = df[target_col].value_counts(normalize=True) * 100\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Count plot\n",
    "    axes[0].bar(target_counts.index, target_counts.values, color=['green', 'red'])\n",
    "    axes[0].set_xlabel('10-Year CHD Risk')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Target Variable Distribution')\n",
    "    axes[0].set_xticks([0, 1])\n",
    "    axes[0].set_xticklabels(['No Risk', 'Risk'])\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(target_counts.values, labels=['No Risk', 'Risk'], \n",
    "                autopct='%1.1f%%', colors=['green', 'red'])\n",
    "    axes[1].set_title('Target Variable Proportion')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Class Distribution:\")\n",
    "    print(f\"No Risk: {target_counts[0]} ({target_percentages[0]:.2f}%)\")\n",
    "    print(f\"Risk: {target_counts[1]} ({target_percentages[1]:.2f}%)\")\n",
    "    print(f\"\\nClass Imbalance Ratio: 1:{target_counts[0]/target_counts[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze numerical features\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target_col in numerical_cols:\n",
    "    numerical_cols.remove(target_col)\n",
    "\n",
    "# Create distribution plots\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols[:16]):\n",
    "    axes[idx].hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'Distribution of {col}')\n",
    "    \n",
    "    # Add mean line\n",
    "    mean_val = df[col].mean()\n",
    "    axes[idx].axvline(mean_val, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean_val:.2f}')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = df[numerical_cols + [target_col]].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(correlation_matrix), k=1)\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='coolwarm', center=0, square=True, linewidths=1,\n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Features most correlated with target\n",
    "target_correlations = correlation_matrix[target_col].sort_values(ascending=False)\n",
    "print(f\"\\nTop 10 Features Correlated with {target_col}:\")\n",
    "print(target_correlations.head(11)[1:])  # Exclude self-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick feature importance using Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare data\n",
    "X = df[numerical_cols].fillna(df[numerical_cols].median())\n",
    "y = df[target_col]\n",
    "\n",
    "# Train a quick Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': numerical_cols,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['feature'][:15], feature_importance['importance'][:15])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Feature Importances (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR method\n",
    "outlier_summary = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    outlier_summary[col] = len(outliers)\n",
    "\n",
    "# Create outlier summary DataFrame\n",
    "outlier_df = pd.DataFrame(list(outlier_summary.items()), \n",
    "                          columns=['Feature', 'Outlier_Count'])\n",
    "outlier_df['Outlier_Percentage'] = (outlier_df['Outlier_Count'] / len(df)) * 100\n",
    "outlier_df = outlier_df.sort_values('Outlier_Percentage', ascending=False)\n",
    "\n",
    "# Plot outlier summary\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(outlier_df['Feature'][:10], outlier_df['Outlier_Percentage'][:10])\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Outlier Percentage (%)')\n",
    "plt.title('Top 10 Features with Outliers')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Outlier Summary (Top 10):\")\n",
    "print(outlier_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPLORATORY DATA ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Dataset Overview:\")\n",
    "print(f\"  ‚Ä¢ Total samples: {len(df)}\")\n",
    "print(f\"  ‚Ä¢ Total features: {df.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Numerical features: {len(numerical_cols)}\")\n",
    "print(f\"  ‚Ä¢ Target variable: {target_col}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Key Findings:\")\n",
    "print(f\"  ‚Ä¢ Class imbalance detected (Risk cases: {target_percentages[1]:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Missing values in {len(missing_df)} features\")\n",
    "print(f\"  ‚Ä¢ Outliers detected in multiple features\")\n",
    "\n",
    "print(\"\\nüí° Recommendations:\")\n",
    "print(\"  1. Handle class imbalance using SMOTE or class weights\")\n",
    "print(\"  2. Impute missing values using appropriate strategies\")\n",
    "print(\"  3. Consider outlier treatment for robust modeling\")\n",
    "print(\"  4. Focus on top correlated features for initial models\")\n",
    "print(\"  5. Apply feature scaling for distance-based algorithms\")\n",
    "print(\"  6. Consider feature engineering for age groups and risk categories\")\n",
    "\n",
    "print(\"\\n‚úÖ Next Steps:\")\n",
    "print(\"  ‚Ä¢ Proceed with data preprocessing\")\n",
    "print(\"  ‚Ä¢ Implement feature engineering\")\n",
    "print(\"  ‚Ä¢ Train multiple models and compare performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
